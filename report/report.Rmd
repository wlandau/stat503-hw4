---
title: "STAT 503 Homework 4: STAT 101 Grades"
author: "Andee Kaplan, Will Landau, Fangge Liu, Lindsay Rutter"
date: "March 27, 2015"
output:
  pdf_document:
    fig_caption: true
bibliography: report.bib
---

```{r libraries, echo = FALSE, message = F}
library(ggplot2)
library(knitr)
```

```{r hooks, echo = F}
opts_chunk$set(echo = FALSE, cache = TRUE, message = FALSE, warning = FALSE)
theme_set(theme_bw(base_family="serif"))
```


```{r load-som, cache=FALSE}
source("../R/som-vis.R") #I am placing this here because I can't get it to run after all other packages are loaded in to the namespace (dplyr, tidyr problems). also may be caching issue
source("../R/som-predict.R")
source("../R/mclust-vis.R")
source("../R/mclust-predict.R")
```

#Introduction

STAT 101 course instructors at Iowa State University usually claim their students are diverse. The undergrads who sign up have a wide variety of majors, backgrounds, perspectives, abilities, and levels of motivation. Visual and unsupervised analyses of homework grades may classify students in useful, insightful ways and even inform pedagogy.

We have three homework grade spreadsheets, each of which comes directly from Blackboard [@blackboard], Iowa State's system for managing course materials and grades. Each dataset corresponds to a single semester of STAT 101: either fall 2013, fall 2014, or spring 2014. Each semester has six or seven sections of roughly one hundred students each. Every spreadsheet has roughly twenty variables, each of which corresponds to a homework grade (either percentage of points earned or NA for a missing assignment) or the average homework score with missing assignments removed.

#Missing values

A large fraction of homework scores appear as "NA", or missing. Since the spreadsheets come directly from Blackboard, we assume that almost all NA's correspond to homeworks that students failed to turn in, and only a small number, if any, are from bookkeeping errors. Before clustering, we need to impute these values, and for an imputation strategy, we look at patterns of missingness. 

###Visual patterns in missing values

Figure \ref{fig:missingHist} plots the number of missing values per student for all students. Most of the students missed few to no assignments, and some students missed several. This pattern is consistent with each semester and section. Figure \ref{fig:missingByAssign} shows the number of missing values for each assignment within each semester and section. For the fall semesters, notable spikes occur at chapter 9 ("Understanding Randomness") and topic 9 ("Sample Surveys"). Otherwise, for most sections, the number of missings increased steadily for each section over time.

Figures \ref{fig:missingBarcodesFall13}, \ref{fig:missingBarcodesFall14}, and \ref{fig:missingBarcodesSpring14} show each student's pattern of missingness. General patterns are consistent. Most students had few missing assignments scattered sporadically over the semester, some students dropped the class early, and a smaller students missed strings of around five or ten consecutive assignments in the beginning, middle, or end of the semester.


```{r missing-vis.R}
source("../R/missing-vis.R")
```

```{r missingHist,  fig.cap="\\label{fig:missingHist} number of missing values per student for all students. The top right panel facets by semester, and the bottom left panel facets by semester and section number. Most of the students missed few to zero assignments, and some students missed several. This pattern is consistent with each semester and section."}
missingHist()
```


```{r missingByAssign, fig.cap="\\label{fig:missingByAssign} number of missing values per assignment for each semester and section. For the fall semesters, notable spikes occur at chapter 9 (\"Understanding Randomness\") and topic 9 (\"Sample Surveys\"). Otherwise, for most sections, the number of missings increased steadily for each section over time."}
missingByAssign()
```

```{r missingBarcodesFall13, fig.cap="\\label{fig:missingBarcodesFall13} missing assignment records for fall 2013 students faceted by section number. Each row represents a student, each column is an assignment, and the tiles are colored according to the status of the corresponding assignment (missing or turned in). General patterns are consistent across section number. Most students missed few assignments, and the students with the most missings usually missed the last fifteen assignments. These students most likely dropped the class early."}
missingBarcodesFall13()
```

```{r missingBarcodesFall14, fig.cap="\\label{fig:missingBarcodesFall14} Same as Figure \\ref{fig:missingBarcodesFall13}, but for fall 2014. We see some early drops, but also some students who failed to turn in either the first few or the middle few assignments."}
missingBarcodesFall14()
```

```{r missingBarcodesSpring14, fig.cap="\\label{fig:missingBarcodesSpring14} Same as Figure \\ref{fig:missingBarcodesFall13}, but for spring 2014. Patterns are similar to those of Figure \\ref{fig:missingBarcodesFall13} (fall 2013)."}
missingBarcodesSpring14()
```

###Grouping students by missingness

From inspection, we can partition the students in each semester into four groups.

Group 1 contains students who did not submit all of the last nine (about half) of all homework assignments. Group 2 contains students (who were not in Group 1) and missed at least nine (about half) of all homework assignments. Group 3 contains students (who were not in Group 1 or 2) and missed at least one homework assignment. Group 4 contains students (who were not in Group 1, 2, or 3) who did not miss any homework assignments.

As a result, we generated 12 main groups (the four groups across three semesters), as shown in Table \ref{tab:MainGroups}.  

```{r GeneralMissingClusters}
source("../R/GeneralMissingClusters.R")
```


```{r, message=FALSE, echo=FALSE}
require(knitr)
require(dplyr)
require(ggplot2)

grp <- matrix(c(nrow(Fall13_G1),nrow(Fall13_G2),nrow(Fall13_G3),nrow(Fall13_G4), nrow(Spring14_G1),nrow(Spring14_G2),nrow(Spring14_G3),nrow(Spring14_G4),nrow(Fall14_G1),nrow(Fall14_G2),nrow(Fall14_G3),nrow(Fall14_G4)), ncol=3)
colnames(grp) <- c('Fall 13', 'Spring 14', 'Fall 14')
rownames(grp) <- c('Group 1 - Drop outs', 'Group 2 - Common missings', 'Group 3 - Sporadic missings', 'Group 4 - No missings')
grp.table <- as.table(grp)

grp.table %>%
  kable(caption = "\\label{tab:MainGroups}The number of students who were categorized into one of four mutually-exclusive groups, for the three semesters. We considered Groups 1 and 2 to be problematic, as they likely represented students who dropped the course or habitually missed assignments. However, even after removing students from Groups 1 and 2, we are still left with a very large dataset to cluster")
```

### Trim and impute

As we explain above, groups 1 and 2 are relatively small, and the students in these groups missed at least half of the homework assignments. With good reason, we give each of these groups its own cluster and concentrate the rest of our analysis on groups 3 and 4 only. These remaining students have some missing values left, and we impute them with the nearest neighbors imputation functionality in the DMwR [@DMwR] package. Dr. Cook wrote most of this code.

```{r impute.R}
source("../R/impute.R") 
```

\clearpage

# A look at the cleaned data

```{r explore.R}
source("../R/explore.R")
```

The parallel coordinate plot in the problem statement shows most students tended to do fairly well on homework, and Chapters 1, 9, 12, and 19 may be nuisance variables because of the lack of variability. Figure \ref{fig:hist} shows histograms of all the homeworks scores (and the average score) for fall 2013. The results for other semesters are similar. It will be difficult to split on any one variable individually except for maybe chapter 12. Chapters 1 and 2, along with the average score, look like nuisance variables due to low variability.

```{r hist, fig.cap="\\label{fig:hist} histograms of all the homeworks scores (and the average score) for fall 2013. The results for other semesters are similar. It will be difficult to split on any one variable individually except for maybe chapter 12. Chapters 1 and 2, along with the average score, look like nuisance variables due to low variability."}
histplot() 
```

Figure \ref{fig:pairsplot} shows a scatterplot matrix of the first ten variables from fall 2013. The results of the full 20 variables are similar. The It will be difficult to split the data on any pair of variables, as no pair provides much of a spatial separation in the data. In addition, we have a lot of nuisance variables: Figure \ref{fig:corhist} shows that the homework scores are often highly correlated.

```{r pairsplot, fig.cap= "\\label{fig:pairsplot} scatterplot matrix of the first ten variables from fall 2013. The results of the full 20 variables are similar. It will be difficult to split the data on any pair of variables, as no pair provides much of a spatial separation in the data."}
pairsplot()
```


```{r corhist, fig.cap = "\\label{fig:corhist} Histograms of correlations among homework scores, facetted by semester. The homework scores are often highly correlated, leading to nuisance variables and causing problems for typical hard clustering methods."}
corplot()
```



The dimensionality of the data makes visualization difficult, so Figure \ref{fig:pca} shows the first two principal components for each semester, and Figure \ref{fig:mds} plots the dimensionality reduction from multidimensional scaling to two dimensions. In both cases, the data do not spatially separate into clusters. Geometrically, each semester's grades coagulate into a blob. It will be difficult, if not impossible, for typical hard clustering techniques, or any other techniques that assume an obvious visual partitioning, to make any headway. The next section briefly attempts these techniques anyway, self-organizing maps and model-based clustering are much more promising directions.


```{r pca, fig.cap="\\label{fig:pca} first two principal components, plotted against each other, for the all semesters. The data do not spatially separate."}
pcaplot()
```

Figure \ref{fig:mds} plots the dimensionality reduction from multidimensional scaling to two dimensions.

```{r mds, results="hide", fig.cap="\\label{fig:mds} first two variables from multidimensional scaling, plotted against each other, for all semesters. The data do not spatially separate."}
mds.plot()
```



# Attempts at typical hard clustering

Here, we try some hard clustering techniques that assume the data separate into clusters spatially, just in case there is a hidden spatial separation that we didn't detect in the previous section. At first, we see if wb.ratio gives us a quick answer for the number of clusters in the data. Figure \ref{fig:wb-ratio} shows wb.ratio as a function of $k$, the number of clusters. We show results for kmeans with Euclidian distance (with the kmeans function in core R), kmeans with correlation distance (in the amap package [@amap]), and hierarchical clustering with several linkage methods (with the hclust function in core R). It is alarming that wb.ratio does not decrease monotonically with $k$. Increasing the number of clusters should improve clustering outcomes. Kmeans with correlation distance is the closest to having monotone decreasing wb.ratio, but it also has the overall highest wb.ratio. This is one indication that the data may not separate. Typical hard clustering just may not work.
```{r num-clusters.R, cache = FALSE}
source("../R/num-clusters.R")
```

```{r wb-ratio-plot, fig.cap="\\label{fig:wb-ratio} wb.ratio as a function of $k$, the number of clusters. We show results for kmeans with Euclidian distance (with the kmeans function in core R), kmeans with correlation distance (in the amap package [@amap]), and hierarchical clustering with several linkage methods (with the hclust function in core R). It is alarming that wb.ratio does not decrease monotonically with $k$. Increasing the number of clusters should improve clustering outcomes. Kmeans with correlation distance is the closest to having monotone decreasing wb.ratio, but it also has the overall highest wb.ratio."}
wb.ratio.plot()
```

We also look at dendrograms from hierarchical clustering to get a sense an optimal $k$, if we can determine $k$ at all. Figure \ref{fig:dendros} shows dendrograms from hierarchical clustering using six linkage methods. Only spring 2014 dendrograms are shown, as results for the other two semesters are similar. The ward linkage dendrograms indicate that $k = 3$ may be reasonable, and the other methods fail to find any meaningful separation.

```{r dendros, fig.cap="\\label{fig:dendros} dendrograms from hierarchical clustering to get a sense an optimal $k$, if we can determine $k$ at all. Figure \ref{fig:dendros} shows dendrograms from hierarchical clustering using six linkage methods. Only spring 2014 dendrograms are shown, as results for the other two semesters are similar. The ward linkage dendrograms indicate that $k = 3$ may be reasonable, and the other methods fail to find any meaningful separation."}
dendros()
```

As it turns out, ward linkage at $k = 3$ gives only a vague separation. Figure \ref{fig:wardParcoord} shows that ward divides students into high, medium, and low performance, but that there's still a ton of variety within clusters. 


Overall, there is little natural spatial separation in the data, and classical hard clustering methods tell a brief, uninteresting story. Self organizing maps and model based clustering may have more to say.


```{r wardParcoord, fig.cap = "\\label{fig:wardParcoord} parallel coordinate plot of the spring 2014 scores, facetted by cluster from hierarchical clustering with ward linkage. Ward divides students into high, medium, and low performance, but that there's still a ton of variety within clusters. Overall, there is little natural spatial separation in the data, and classical hard clustering methods tell a brief, uninteresting story. Self organizing maps and model based clustering may have more to say."}
ward.parcoord()
```

#Self Organizing Maps

As another attempt to cluster the students by homework grades, we fit multiple self-organizing maps (SOM) using the `kohonen` package [@kohonen] and examined the `wb.ratio` to determine how many nodes should be in the SOM grid. We examined grids of size $2\times1$ up to $15\times15$ and visualized the `wb.ratio` by grid, as well as by total number of nodes in figures \ref{fig:som_ratio1} and \ref{fig:som_ratio2}. Each model is fit using 2000 iterations. 

```{r som_ratio1, fig.cap = "\\label{fig:som_ratio1} A plot of `wb.ratio` by grid values in the self organizing maps that were fit to homework grades. We focus on fall 2013 in order to create a map that can be applied to the other semesters. There is a sharp decline in `wb.ratio` diagonally across the grids."}
som.plot.wb.ratio
```


```{r som_ratio2, fig.cap = "\\label{fig:som_ratio2} A plot of `wb.ratio` by total number of nodes in the self organizing maps that were fit to homework grades. We focus on fall 2013 in order to create a map that can be applied to the other semesters. There is a sharp decline in `wb.ratio` around 36 nodes in fall 2014 before the ratio levels off. As such, we will work with the $6 \times 6$ SOM.", fig.height=3}
som.plot.wb.ratio.line
```

After looking at the `wb.ratio` across the grids and across the total number of nodes, paying special attention to fall 2013, we notice a sharp decline in the ratio at around 36 nodes, or a $6\times6$ gridded SOM model. Thus, we choose the $6\times6$ gridded SOM model and map spring 2014 and fall 2014 to our fitted map. However, before mapping our future semesters to the SOM, we first looked at our clustering in the fall semester (Figure \ref{fig:som_cluster}) using a facetted parallel coordinate plot as well as checked convergence (figure \ref{fig:som_converge}) looking at the mean distance to the closest SOM unit during training.

```{r som_converge, fig.cap="\\label{fig:som_converge} Convergence of our choses $6\\times6$ SOM model over 2000 iterations, looking at the mean distance to the closest SOM unit during training. It appears we have a steady convergence at around 600 iterations.", fig.height=3}
plot(chosen.som[[1]][[1]], "changes")
```

Looking at the mean distance to the closest SOM unit during training. It appears we have a steady convergence at around 600 iterations, which means we have run our chosen SOM model enough iterations during fitting.

```{r som_cluster, fig.cap="\\label{fig:som_cluster} Clustering in the fall semester of our SOM model using a facetted parallel coordinate plot. Students separate by general performance and consistency. For example, cluster 31 has the most high performing and consistent students. However, our insight is even more specific. Some clusters, like 3 and 19, extract students who all had high relative difficulty with some cluster-specific assignment. Cluster 5 shows students who did well on the first assignment, declined in performance, boosted their grades during the middle of the semester, and then declined again at the end.", fig.height=7}
som.plot.parcoord
```



Figure \ref{fig:som_cluster} shows that students separate by general performance and consistency. For example, cluster 31 has the most high performing and consistent students. However, our insight is even more specific. Some clusters, like 3 and 19, extract students who all had high relative difficulty with some cluster-specific assignment. Cluster 5 shows students who did well on the first assignment, declined in performance, boosted their grades during the middle of the semester, and then declined again at the end. Some clusters, however, like cluster 24, still have high within-cluster variability.

Now we can fit our other semesters data, spring 2014 and fall 2014, to the chosen fall 2013 SOM model and see if we have similar patterns in the grades. But first, we must process the data to be in the same form as fall 2014, i.e. number of columns and column names. To accomplish this, we use the mapping given in the problem assignment and use simple averages for scores from topics that multiply map to fall 2013 topics.

- Chapter 1, Topic01: What is statistics?
- Chapter 2, Topic02, Topic04: Displaying and Describing Categorical Data
- Chapter 3, Topic03: Displaying and Summarizing Quantitative Data
- Chapter 4, Topic05: Understanding and Comparing Distributions
- Chapter 5, Topic06: The Standard Deviation as a Ruler and The Normal Model
- Chapter 6, Topic07: Scatterplots, Association, Correlation
- Chapter 7,8, Topic08: Linear Regression, Regression Wisdom
- Chapter 9, Topic11: Understanding Randomness
- Chapter 10, Topic09: Sample Surveys
- Chapter 11, Topic10: Experiments
- Chapter 12, Topic12: From Randomness to Probability
- Chapter 15I, Topic15: Sampling Distribution Models - Proportions
- Chapter 16, Topic16: Confidence Intervals for Proportions
- Chapter 17, Topic17: Testing Hypotheses about Proportions
- Chapter 15II, Topic18: Sampling Distribution Models - Means
- Chapter 18, Topic19: Inferences About Means
- Chapter 19, Topic20: More About Tests and Intervals
- Chapter 20, Topic21, Topic22, Topic23: Comparing Groups

Additionally, Chapter 12: From Randomness to Probability was missing from both spring 2014 and fall 2014 and  Chapter 18: Inferences About Means and Chapter 19: More about Tests and Intervals were both missing from fall 2014. Thus, we threw these variables out of the $6 \times 6$ SOM model and refit before prediction. The results of predicting our new data can be seen in figure \ref{fig:som_predict}.

```{r som-predict, fig.cap="\\label{fig:som_predict}Prediction of spring 2014 and fall 2014 clusters based on the equivalent mapping of homework topics by mapping the data onto the (modified) trained $6\\times6$ SOM model. Nodes 34 and 35 no longer have any members in the cluster for this new data.", fig.height=7}
som.plot.parcoord.predict
```

From figure \ref{fig:som_predict}, we do see some clusters that remain intact in the new semesters, for example cluster 7 appears to be those students that performed very poorly on Chapter 5: The Standard Deviation as a Ruler and The Normal Model. Additionally, clusters 5, 6, and 9 seem to have decent patterns within them. However, overall there is a lot of variation within the clusters, showing that this may not be a very good model for prediction. Additionally, we can look at the `wb.ratio` for this new data, the value of `r round(som.predict.wb.ratio, 3)`, which is higher than the `wb.ratio`s for fall 2013. This poor prediction leads us to try and perform model based clustering.

#Model based clustering
First we fit the model to fall 13 to select a best specification. Figure \ref{fig:mc_BIC} shows the BIC of different specifications. From the plot, the VII specification with five clusters seems to be the sensible choice which is shown in figure \ref{fig:mc_fall13}. The value of the `wb.ratio` is `r round(mclust.wb.ratio, 3)`. 

```{r fig:mc_BIC, fig.cap="\\label{fig:mc_BIC}Plot of BIC for different models and numbers of clusters.", fig.height=7}
plot(fall13.mc, what="BIC")
```

```{r mc_fall13, fig.cap="\\label{fig:mc_fall13} parallel coordinate plot of our model based clustering of the fall 2013 students. The model separates students by general level of success and shows that the most successful students have most consistent scores, but other than that, the results do not offer much specific insight.", fig.height=7}
mclust.plot.parcoord
```

Figure \ref{fig:mc_fall13} in general, model separates students by overall success and consistency. Cluster 1 has the most successful and consistent students, and cluster 5 has the least successful, least consistent students. Other than that, however, the results do not offer much insight into similarities and differences among students.  

Figure \ref{fig:mean_mc_fall13} shows the mean score for each assignment by cluster. Although it does not show deviations from the mean, we can still see some consistent pattern across clusters like the dive in mean grade for Ch3 and Ch15I, possibly a sign of difficult than usual assignments. The mean is decreasing from cluster 2 to cluster 5, except for some cases where cluster 5 does better than cluster 4 in terms of mean grade. And there is more variance across chapters for cluster 5, which is consistent with results of clustering. 

```{r mean_mc_fall13, fig.cap="\\label{fig:mean_mc_fall13}Plot of mean scores for each assignment by clusters. Again, the model-based approach separates students by consistency and overall performance. This plot is mostly useful as a profile of general levels of homework difficulty. For example, homework 1 was the easiest for most groups on average.", fig.height=7}
mclust.plot.mean.parcoord
```
 
Figure \ref{fig:mc_pca} shows the principle component analysis. There are considerable overlappings between cluster 1 and 2, 2 and 3, and 4 and 5, suggesting rooms for improvement in terms of clustering. But we can still see patterns consistent with the parallel coordinate plot. 

```{r mc_pca, fig.cap="\\label{fig:mc_pca}Clustering for principle components.", fig.height=7}
mclust.plot.pca
```

We also fit our other semesters data, spring 2014 and fall 2014, to the chosen fall 2013 MC model like we did for the SOM model. We formatted the data in the same way. The results are shown in figure \ref{fig:mc_predict}. As we can see, the cluster retain for the new data, especially for fall 14. Figure \ref{fig:mc_mean_predict} shows the mean score for each assignment by cluster for the new data. It is pretty consistent with fall 13 data, including the dives in particular assignments and the order of mean grades across clusters. Figure \ref{fig:mc_pca_predict} shows the principle component analysis for the new data.In general the model based clustering does do a better job than SOM, and there are similar patterns in grades across semesters. 

```{r mc_predict, fig.cap="\\label{fig:mc_predict}Prediction of spring 2014 and fall 2014 clusters based on the equivalent mapping of homework topics by mapping the data onto the trained model.", fig.height=7}
mc.plot.parcoord.predict
```


```{r mc_mean_predict, fig.cap="\\label{fig:mc_mean_predict}Plot of mean scores for each assignment by clusters for new data.", fig.height=7}
mclust.plot.mean.parcoord.predict
```

```{r mc_pca_predict, fig.cap="\\label{fig:mc_pca_predict}Clustering for principle components for new data.", fig.height=7}
mclust.plot.pca.pred
```



#Acknowledgements

We would like to thank Dr. Cook for her advice on dealing with missing values and her imputation code. Also, we used the R packages amap [@amap], cluster [@cluster], DMwR [@DMwR], fpc [@fpc], gdata [@gdata], ggplot2 [@ggplot2], gridExtra [@gridExtra], reshape2 [@reshape2], and vegan [@vegan].

#References

