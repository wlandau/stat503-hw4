---
title: "STAT 503 Homework 4: STAT 101 Grades"
author: "Andee Kaplan, Will Landau, Fangge Liu, Lindsay Rutter"
date: "March 27, 2015"
output:
  pdf_document:
    fig_caption: true
bibliography: report.bib
---

```{r libraries, echo = FALSE, message = F}
library(ggplot2)
library(knitr)

```

```{r hooks, echo = F}
opts_chunk$set(echo = F, cache = T, message = F, warning = F)
theme_set(theme_bw(base_family="serif"))
```

#Introduction

STAT 101 course instructors at Iowa State University usually claim their students are diverse. The undergrads who sign up have a wide variety of majors, backgrounds, perspectives, abilities, and levels of motivation. Visual and unsupervised analyses of homework grades may classify students in useful, insightful ways and even inform pedagogy.

We have three homework grade spreadsheets, each of which comes directly from Blackboard [@blackboard], Iowa State's system for managing course materials and grades. Each dataset corresponds to a single semester of STAT 101: either fall 2013, fall 2014, or spring 2014. Each semester has six or seven sections of roughly one hundred students each. Every spreadsheet has roughly twenty variables, each of which corresponds to a homework grade (either percentage of points earned or NA for a missing assignment) or the average homework score with missing assignments removed.

#Missing values

A large fraction of homework scores appear as "NA", or missing. Since the spreadsheets come directly from Blackboard, we assume that almost all NA's correspond to homeworks that students failed to turn in, and only a small number, if any, are from bookkeeping errors. Before clustering, we need to impute these values, and for an imputation strategy, we look at patterns of missingness. 

###Visual patterns in missing values

Figure \ref{fig:missingHist} plots the number of missing values per student for all students. Most of the students missed few to no assignments, and some students missed several. This pattern is consistent with each semester and section. Figure \ref{fig:missingByAssign} shows the number of missing values for each assignment within each semester and section. For the fall semesters, notable spikes occur at chapter 9 ("Understanding Randomness") and topic 9 ("Sample Surveys"). Otherwise, for most sections, the number of missings increased steadily for each section over time.

Figures \ref{fig:missingBarcodesFall13}, \ref{fig:missingBarcodesFall14}, and \ref{fig:missingBarcodesSpring14} show each student's pattern of missingness. General patterns are consistent. Most students had few missing assignments scattered sporadically over the semester, some students dropped the class early, and a smaller students missed strings of around five or ten consecutive assignments in the beginning, middle, or end of the semester.


```{r missing-vis.R}
source("../R/missing-vis.R")
```

```{r missingHist,  fig.cap="\\label{fig:missingHist} number of missing values per student for all students. The top right panel facets by semester, and the bottom left panel facets by semester and section number. Most of the students missed few to zero assignments, and some students missed several. This pattern is consistent with each semester and section."}
missingHist()
```


```{r missingByAssign, fig.cap="\\label{fig:missingByAssign} number of missing values per assignment for each semester and section. For the fall semesters, notable spikes occur at chapter 9 (\"Understanding Randomness\") and topic 9 (\"Sample Surveys\"). Otherwise, for most sections, the number of missings increased steadily for each section over time."}
missingByAssign()
```

```{r missingBarcodesFall13, fig.cap="\\label{fig:missingBarcodesFall13} missing assignment records for fall 2013 students faceted by section number. Each row represents a student, each column is an assignment, and the tiles are colored according to the status of the corresponding assignment (missing or turned in). General patterns are consistent across section number. Most students missed few assignments, and the students with the most missings usually missed the last fifteen assignments. These students most likely dropped the class early."}
missingBarcodesFall13()
```

```{r missingBarcodesFall14, fig.cap="\\label{fig:missingBarcodesFall14} Same as Figure \\ref{fig:missingBarcodesFall13}, but for fall 2014. We see some early drops, but also some students who failed to turn in either the first few or the middle few assignments."}
missingBarcodesFall14()
```

```{r missingBarcodesSpring14, fig.cap="\\label{fig:missingBarcodesSpring14} Same as Figure \\ref{fig:missingBarcodesFall13}, but for spring 2014. Patterns are similar to those of Figure \\ref{fig:missingBarcodesFall13} (fall 2013)."}
missingBarcodesSpring14()
```

###Grouping students by missingness

From inspection, we can partition the students in each semester into four groups.

Group 1 contains students who did not submit all of the last nine (about half) of all homework assignments. Group 2 contains students (who were not in Group 1) and missed at least nine (about half) of all homework assignments. Group 3 contains students (who were not in Group 1 or 2) and missed at least one homework assignment. Group 4 contains students (who were not in Group 1, 2, or 3) who did not miss any homework assignments.

As a result, we generated 12 main groups (the four groups across three semesters), as shown in Table \ref{tab:MainGroups}.  

```{r GeneralMissingClusters}
source("../R/GeneralMissingClusters.R")
```


```{r, message=FALSE, echo=FALSE}
library(knitr)
library(dplyr)
library(ggplot2)

grp <- matrix(c(nrow(Fall13_G1),nrow(Fall13_G2),nrow(Fall13_G3),nrow(Fall13_G4), nrow(Spring14_G1),nrow(Spring14_G2),nrow(Spring14_G3),nrow(Spring14_G4),nrow(Fall14_G1),nrow(Fall14_G2),nrow(Fall14_G3),nrow(Fall14_G4)), ncol=3)
colnames(grp) <- c('Fall 13', 'Spring 14', 'Fall 14')
rownames(grp) <- c('Group 1 - Drop outs', 'Group 2 - Common missings', 'Group 3 - Sporadic missings', 'Group 4 - No missings')
grp.table <- as.table(grp)

grp.table %>%
  kable(caption = "\\label{tab:MainGroups}The number of students who were categorized into one of four mutually-exclusive groups, for the three semesters. We considered Groups 1 and 2 to be problematic, as they likely represented students who dropped the course or habitually missed assignments. However, even after removing students from Groups 1 and 2, we are still left with a very large dataset to cluster")
```

### Trim and impute

As we explain above, groups 1 and 2 are relatively small, and the students in these groups missed at least half of the homework assignments. With good reason, we give each of these groups its own cluster and concentrate the rest of our analysis on groups 3 and 4 only. These remaining students have some missing values left, and we impute them with the nearest neighbors imputation functionality in the DMwR [@DMwR] package. Dr. Cook wrote most of this code.

```{r impute.R}
source("../R/impute.R") 
```

\clearpage

# A look at the cleaned data

```{r explore.R}
source("../R/explore.R")
```

The parallel coordinate plot in the problem statement shows most students tended to do fairly well on homework, and Chapters 1, 9, 12, and 19 may be nuisance variables because of the lack of variability. Figure \ref{fig:hist} shows histograms of all the homeworks scores (and the average score) for fall 2013. The results for other semesters are similar. It will be difficult to split on any one variable individually except for maybe chapter 12. Chapters 1 and 2, along with the average score, look like nuisance variables due to low variability.

```{r hist, fig.cap="\\label{fig:hist} histograms of all the homeworks scores (and the average score) for fall 2013. The results for other semesters are similar. It will be difficult to split on any one variable individually except for maybe chapter 12. Chapters 1 and 2, along with the average score, look like nuisance variables due to low variability."}
histplot() 
```

Figure \ref{fig:pairsplot} shows a scatterplot matrix of the first ten variables from fall 2013. The results of the full 20 variables are similar. The It will be difficult to split the data on any pair of variables, as no pair provides much of a spatial separation in the data. In addition, we have a lot of nuisance variables: Figure \ref{fig:corhist} shows that the homework scores are often highly correlated.

```{r pairsplot, fig.cap= "\\label{fig:pairsplot} scatterplot matrix of the first ten variables from fall 2013. The results of the full 20 variables are similar. It will be difficult to split the data on any pair of variables, as no pair provides much of a spatial separation in the data."}
pairsplot()
```


```{r corhist, fig.cap = "\\label{fig:corhist} Histograms of correlations among homework scores, facetted by semester. The homework scores are often highly correlated, leading to nuisance variables and causing problems for typical hard clustering methods."}
corplot()
```



The dimensionality of the data makes visualization difficult, so Figure \ref{fig:pca} shows the first two principal components for each semester, and Figure \ref{fig:mds} plots the dimensionality reduction from multidimensional scaling to two dimensions. In both cases, the data do not spatially separate into clusters. Geometrically, each semester's grades coagulate into a blob. It will be difficult, if not impossible, for typical hard clustering techniques, or any other techniques that assume an obvious visual partitioning, to make any headway. The next section briefly attempts these techniques anyway, self-organizing maps and model-based clustering are much more promising directions.


```{r pca, fig.cap="\\label{fig:pca} first two principal components, plotted against each other, for the all semesters. The data do not spatially separate."}
pcaplot()
```

Figure \ref{fig:mds} plots the dimensionality reduction from multidimensional scaling to two dimensions.

```{r mds, results="hide", fig.cap="\\label{fig:mds} first two variables from multidimensional scaling, plotted against each other, for all semesters. The data do not spatially separate."}
mds.plot()
```



# Attempts at typical hard clustering

Here, we try some hard clustering techniques that assume the data separate into clusters spatially, just in case there is a hidden spatial separation that we didn't detect in the previous section. At first, we see if wb.ratio gives us a quick answer for the number of clusters in the data. Figure \ref{fig:wb-ratio} shows wb.ratio as a function of $k$, the number of clusters. We show results for kmeans with Euclidian distance (with the kmeans function in core R), kmeans with correlation distance (in the amap package [@amap]), and hierarchical clustering with several linkage methods (with the hclust function in core R). It is alarming that wb.ratio does not decrease monotonically with $k$. Increasing the number of clusters should improve clustering outcomes. Kmeans with correlation distance is the closest to having monotone decreasing wb.ratio, but it also has the overall highest wb.ratio. This is one indication that the data may not separate. Typical hard clustering just may not work.
```{r num-clusters.R}
source("../R/num-clusters.R")
```

```{r wb-ratio-plot, fig.cap="\\label{fig:wb-ratio} wb.ratio as a function of $k$, the number of clusters. We show results for kmeans with Euclidian distance (with the kmeans function in core R), kmeans with correlation distance (in the amap package [@amap]), and hierarchical clustering with several linkage methods (with the hclust function in core R). It is alarming that wb.ratio does not decrease monotonically with $k$. Increasing the number of clusters should improve clustering outcomes. Kmeans with correlation distance is the closest to having monotone decreasing wb.ratio, but it also has the overall highest wb.ratio."}
wb.ratio.plot()
```

We also look at dendrograms from hierarchical clustering to get a sense an optimal $k$, if we can determine $k$ at all. Figure \ref{fig:dendros} shows dendrograms from hierarchical clustering using six linkage methods. Only spring 2014 dendrograms are shown, as results for the other two semesters are similar. The ward linkage dendrograms indicate that $k = 3$ may be reasonable, and the other methods fail to find any meaningful separation.

```{r dendros, fig.cap="\\label{fig:dendros} dendrograms from hierarchical clustering to get a sense an optimal $k$, if we can determine $k$ at all. Figure \ref{fig:dendros} shows dendrograms from hierarchical clustering using six linkage methods. Only spring 2014 dendrograms are shown, as results for the other two semesters are similar. The ward linkage dendrograms indicate that $k = 3$ may be reasonable, and the other methods fail to find any meaningful separation."}
dendros()
```

As it turns out, ward linkage at $k = 3$ gives only a vague separation. Figure \ref{fig:wardParcoord} shows that ward divides students into high, medium, and low performance, but that there's still a ton of variety within clusters. 


Overall, there is little natural spatial separation in the data, and classical hard clustering methods tell a brief, uninteresting story. Self organizing maps and model based clustering may have more to say.


```{r wardParcoord, fig.cap = "\\label{fig:wardParcoord} parallel coordinate plot of the spring 2014 scores, facetted by cluster from hierarchical clustering with ward linkage. Ward divides students into high, medium, and low performance, but that there's still a ton of variety within clusters. Overall, there is little natural spatial separation in the data, and classical hard clustering methods tell a brief, uninteresting story. Self organizing maps and model based clustering may have more to say."}
ward.parcoord()
```




#Acknowledgements

We would like to thank Dr. Cook for her advice on dealing with missing values and her imputation code. Also, we used the R packages amap [@amap], cluster [@cluster], DMwR [@DMwR], fpc [@fpc], gdata [@gdata], ggplot2 [@ggplot2], gridExtra [@gridExtra], reshape2 [@reshape2], and vegan [@vegan].

#References

